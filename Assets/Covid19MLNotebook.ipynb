{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fitted-folder",
   "metadata": {},
   "source": [
    "### Get a Role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-cycling",
   "metadata": {},
   "source": [
    "We need to setup the role to access our data from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is from kaggle: https://www.kaggle.com/pranavraikokte/covid19-image-dataset. Upload it on your S3 bucket\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import os\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "# Fill your bucket name\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# Replace it to your S3 Bucket Here\n",
    "BUCKET_NAME = \"fnandito-covid19-data\"\n",
    "prefix = 'lstFile'\n",
    "\n",
    "print('using bucket %s'%BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-notebook",
   "metadata": {},
   "source": [
    "### Define the Label of Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFileArr = []\n",
    "testFileArr = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-bench",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isCovid(DirName):\n",
    "    if DirName.find(\"Covid\") != -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isPneumonia(DirName):\n",
    "    if DirName.find(\"Pneumonia\") != -1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-tomato",
   "metadata": {},
   "source": [
    "### Read Image Files from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-district",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNameDirFromS3(bucketName, remoteDirectoryName):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucketName)\n",
    "    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n",
    "        FileName =  obj.key.split(\"/\")\n",
    "        Label = FileName[2]\n",
    "        Fn = Label + \"/\" + FileName[3]\n",
    "        if remoteDirectoryName == \"Covid19-dataset/train/\":\n",
    "            trainFileArr.append({'Name': Fn ,'LabelCovid': isCovid(Label), 'LabelPneumonia': isPneumonia(Label)})\n",
    "        else:\n",
    "            testFileArr.append({'Name': Fn ,'LabelCovid': isCovid(Label), 'LabelPneumonia': isPneumonia(Label)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetNameDirFromS3(BUCKET_NAME, \"Covid19-dataset/train/\")\n",
    "GetNameDirFromS3(BUCKET_NAME, \"Covid19-dataset/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFile = pd.DataFrame(trainFileArr, columns=['Name','LabelCovid','LabelPneumonia'])\n",
    "testFile = pd.DataFrame(testFileArr, columns=['Name','LabelCovid','LabelPneumonia'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-wilderness",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-stream",
   "metadata": {},
   "source": [
    "### Data Train Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-sword",
   "metadata": {},
   "source": [
    "This step, is where we analyse the amount of data we are having, to see how much the ratio we have for each classes (Covid, Pneumonia, or Normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enhanced-freedom",
   "metadata": {},
   "outputs": [],
   "source": [
    "CovidLabel = trainFile.groupby('LabelCovid').count()\n",
    "PneumoniaLabel = trainFile.groupby('LabelPneumonia').count()\n",
    "TotalData = CovidLabel[\"Name\"][0] + CovidLabel[\"Name\"][1]\n",
    "TotalTrainCovid = CovidLabel[\"Name\"][1]\n",
    "TotalTrainPneumonia = PneumoniaLabel[\"Name\"][1]\n",
    "TotalTrainNormal = TotalData - TotalTrainCovid - TotalTrainPneumonia\n",
    "print(\"Total Train Data : \",TotalData)\n",
    "print(\"Total Train Covid Data : \",TotalTrainCovid)\n",
    "print(\"Total Train Pneumonia Data : \",TotalTrainPneumonia)\n",
    "print(\"Total Train Normal Data : \",TotalTrainNormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "AxisYBar = [TotalTrainNormal, TotalTrainCovid, TotalTrainPneumonia]\n",
    "AxisXBar = [\"TotalNormal\", \"TotalCovid\", \"TotalPneumonia\"]\n",
    "\n",
    "plt.bar(AxisXBar, AxisYBar)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-stanford",
   "metadata": {},
   "source": [
    "### Create LST File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-healthcare",
   "metadata": {},
   "source": [
    "LST File is necessary for Labeling the image and acts as a reference to your machine learning to gather the data. Since this is a multi-class classification problem, we need 2 labels that represents for existense of covid, or pneumonia.\n",
    "\n",
    "There will be 3 combinations available:\n",
    "1 0 label for positive covid patient\n",
    "0 1 label for positive pneumonia patient\n",
    "0 0 label for normal patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-beast",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TrainImageList.lst', 'w') as fp:\n",
    "    for index, df in trainFile.iterrows():\n",
    "        fp.write(str(index) + '\\t')\n",
    "        fp.write(str(df[\"LabelCovid\"]) + '\\t')\n",
    "        fp.write(str(df[\"LabelPneumonia\"]) + '\\t')\n",
    "        fp.write(df[\"Name\"])\n",
    "        fp.write('\\n')\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-partner",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TestImageList.lst', 'w') as fp:\n",
    "    for index, df in testFile.iterrows():\n",
    "        fp.write(str(index) + '\\t')\n",
    "        fp.write(str(df[\"LabelCovid\"]) + '\\t')\n",
    "        fp.write(str(df[\"LabelPneumonia\"]) + '\\t')\n",
    "        fp.write(df[\"Name\"])\n",
    "        fp.write('\\n')\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-reasoning",
   "metadata": {},
   "source": [
    "## Upload LST file to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bridal-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3train_lst = 's3://{}/{}/train_lst/'.format(BUCKET_NAME, prefix)\n",
    "s3validation_lst = 's3://{}/{}/validation_lst/'.format(BUCKET_NAME, prefix)\n",
    "\n",
    "# upload the lst files to train_lst and validation_lst channels\n",
    "!aws s3 cp TrainImageList.lst $s3train_lst --quiet\n",
    "!aws s3 cp TestImageList.lst $s3validation_lst --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "union-terrain",
   "metadata": {},
   "source": [
    "## Create estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-chambers",
   "metadata": {},
   "source": [
    "To create estimator, first we need to create the image for our model to be deployed. Then, estimator will be used to create the machine learning. It will provision a specific server for training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-circus",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")\n",
    "print (training_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/output'.format(BUCKET_NAME)\n",
    "multilabel_ic = sagemaker.estimator.Estimator(training_image,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.p2.xlarge',\n",
    "                                         train_volume_size = 50,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-crown",
   "metadata": {},
   "source": [
    "### Set the data Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "challenging-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3train = \"s3://{}/{}/train/\".format(BUCKET_NAME, \"Covid19-dataset\")\n",
    "s3validation = \"s3://{}/{}/test/\".format(BUCKET_NAME, \"Covid19-dataset\")\n",
    "\n",
    "train_data = sagemaker.session.s3_input(s3train, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data = sagemaker.session.s3_input(s3validation, distribution='FullyReplicated', \n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "                             \n",
    "train_data_lst = sagemaker.session.s3_input(s3train_lst, distribution='FullyReplicated', \n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "validation_data_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated', \n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n",
    "                 'validation_lst': validation_data_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-reputation",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-dealing",
   "metadata": {},
   "source": [
    "Hyperparameters are being used ton determine the characteristics of your model. Assume this is a configuration for your model, how do you want your setup your model. Several configurations required to optimize the result of Machine Learning, so it will deliver better result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-latino",
   "metadata": {},
   "source": [
    "Sometimes, there are something called \"Hyperparameter Optimization (HPO)\", meaning there will be a statistics process behind where it will find the most optimum configuration for a machine learning, by calculating the highest accuracy, and the least amount of loss (difference between predicted and actual label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic.set_hyperparameters(num_layers=18,\n",
    "                             use_pretrained_model=1,\n",
    "                             image_shape = \"3,224,224\",\n",
    "                             num_classes=2,\n",
    "                             mini_batch_size=16,\n",
    "                             resize=256,\n",
    "                             epochs=5,\n",
    "                             learning_rate=0.001,\n",
    "                             num_training_samples=2500,\n",
    "                             use_weighted_loss=1,\n",
    "                             augmentation_type = 'crop_color_transform',\n",
    "                             precision_dtype='float32',\n",
    "                             multi_label=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-button",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "representative-berlin",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_ic.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-guyana",
   "metadata": {},
   "source": [
    "On the top of blue logs, if you see Epoch[0] until Epoch[4]. It means that the machine learning is being trained 5 times using the same data, based on the hyperparameter epoch=5 at the code above. Check the Train-accuracy and Validation-accuracy and see what do you get from there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-calvin",
   "metadata": {},
   "source": [
    "### Create Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-reflection",
   "metadata": {},
   "source": [
    "Once the machine learning is created and trained, we need to deploy it. It will create a separate server with size of m4.xlarge, specifically for your machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_classifier = multilabel_ic.deploy(initial_instance_count = 1,\n",
    "                                          instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-complex",
   "metadata": {},
   "source": [
    "### Create Endpoint Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-appreciation",
   "metadata": {},
   "source": [
    "Endpoint Configuration is being used to deploy your machine learning. Before continue running this code, please go to SageMaker console, and copy the name of your model name on your SageMaker Console, in Inference -> Model. Then change 'ModelName' with your model name on ProdictionVariants Array.\n",
    "\n",
    "you can also change the endpoint_config_name at the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "sage = boto3.Session().client(service_name='sagemaker') \n",
    "\n",
    "endpoint_config_name = \"COVID19-ML-Config\"\n",
    "endpoint_config_response = sage.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType':'ml.m4.xlarge',\n",
    "        'InitialInstanceCount':1,\n",
    "        'ModelName':\"image-classification-2021-04-12-14-48-45-751\",\n",
    "        'VariantName':'AllTraffic'}])\n",
    "\n",
    "print('Endpoint configuration name: {}'.format(endpoint_config_name))\n",
    "print('Endpoint configuration arn:  {}'.format(endpoint_config_response['EndpointConfigArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfactory-prediction",
   "metadata": {},
   "source": [
    "### Create Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-minneapolis",
   "metadata": {},
   "source": [
    "Once the endpoint config is created, we need to create the endpoint. You can change the endpoint_name on the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = \"COVID19-ML-Endpoint\"\n",
    "\n",
    "endpoint_params = {\n",
    "    'EndpointName': endpoint_name,\n",
    "    'EndpointConfigName': endpoint_config_name,\n",
    "}\n",
    "sagemaker = boto3.client(service_name='sagemaker')\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "print('EndpointArn = {}'.format(endpoint_response['EndpointArn']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-president",
   "metadata": {},
   "source": [
    "### Check Endpoint Status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-trading",
   "metadata": {},
   "source": [
    "Once the above code is being executed, we can check the status of the endpoint, since it might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the status of the endpoint\n",
    "response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = response['EndpointStatus']\n",
    "print('EndpointStatus = {}'.format(status))\n",
    "\n",
    "# wait until the status has changed\n",
    "sagemaker.get_waiter('endpoint_in_service').wait(EndpointName=endpoint_name)\n",
    "\n",
    "# print the status of the endpoint\n",
    "endpoint_response = sagemaker.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = endpoint_response['EndpointStatus']\n",
    "print('Endpoint creation ended with EndpointStatus = {}'.format(status))\n",
    "\n",
    "if status != 'InService':\n",
    "    raise Exception('Endpoint creation failed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-junction",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-marketing",
   "metadata": {},
   "source": [
    "To Evaluate, we need to download the test image here, and take a several sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-sigma",
   "metadata": {},
   "source": [
    "### Download Dataset from S3 to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownloadfromS3(bucketName, remoteDirectoryName):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucketName)\n",
    "    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n",
    "        if not os.path.exists(os.path.dirname(obj.key)):\n",
    "            os.makedirs(os.path.dirname(obj.key))\n",
    "        bucket.download_file(obj.key, obj.key) # save to same path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-anxiety",
   "metadata": {},
   "outputs": [],
   "source": [
    "DownloadfromS3(BUCKET_NAME, 'Covid19-dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lyric-medication",
   "metadata": {},
   "source": [
    "Take image sample and evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-complexity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_name_covid = './Covid19-dataset/test/Covid/0100.jpeg'\n",
    "file_name_normal = './Covid19-dataset/test/Normal/0101.jpeg'\n",
    "file_name_pneumonia = './Covid19-dataset/test/Viral Pneumonia/0101.jpeg'\n",
    "\n",
    "with open(file_name_covid, 'rb') as image:\n",
    "    f_covid = image.read()\n",
    "    \n",
    "with open(file_name_normal, 'rb') as image:\n",
    "    f_normal = image.read()\n",
    "    \n",
    "with open(file_name_pneumonia, 'rb') as image:\n",
    "    f_pneumonia = image.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "runtime = boto3.Session().client(service_name='runtime.sagemaker')\n",
    "response_covid = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=f_covid)\n",
    "\n",
    "response_normal = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=f_normal)\n",
    "\n",
    "response_pneumonia = runtime.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/x-image', \n",
    "                                   Body=f_pneumonia)\n",
    "\n",
    "result_covid = response_covid['Body'].read()\n",
    "result_normal = response_normal['Body'].read()\n",
    "result_pneumonia = response_pneumonia['Body'].read()\n",
    "# result will be in json format and convert it to ndarray\n",
    "result_covid = json.loads(result_covid)\n",
    "result_normal = json.loads(result_normal)\n",
    "result_pneumonia = json.loads(result_pneumonia)\n",
    "# the result will output the probabilities for all classes\n",
    "\n",
    "# find the class with maximum probability and print the class index\n",
    "index_covid = np.argmax(result_covid)\n",
    "index_normal = np.argmax(result_normal)\n",
    "index_pneumonia = np.argmax(result_pneumonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "christian-season",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_covid)\n",
    "print(result_normal)\n",
    "print(result_pneumonia)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
